{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d1f8d7-a32d-4d64-aaee-d0b0fe0b7450",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb520822-b289-433b-b6ca-f188c00ee31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from datasets import Dataset, load_from_disk, DatasetDict\n",
    "import evaluate\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoConfig\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from metrics_wrapper import MetricsWrapper\n",
    "\n",
    "\n",
    "import math\n",
    "import sqlite3\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115bacc4-e14d-4770-bdb2-b4256bcbce4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import data and create ðŸ¤— dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f754958-ae04-43cd-8eeb-9e67f03b9417",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_df = pd.read_csv(\"data/flashback_annotated.tsv\", sep=\"\\t\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d3c462-bda0-46a4-8616-ecb36a78272d",
   "metadata": {},
   "source": [
    "My reasoning for the split is as follows: I need the training set to be as large as possible, seeing that I want to give it more than a fair chance. In order for me to do that I need to have the split be large enough to accomodate another dev-split for the training set, in addition to the 15% taken for the test set. Additionally, I'll have to define a function which gives the test/train set the same proportion as the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6e61ccf5-17a0-466c-9254-1fd335338706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split(df: str, train_size=0.8, test_size=0.2):\n",
    "    \"\"\" splits a dataframe to two randomly according to proportion of labels \"\"\"\n",
    "    assert test_size + train_size == 1\n",
    "    np.random.seed(666)  # for reproducibility\n",
    "    res = dict()\n",
    "    SIZE = len(df)\n",
    "    PROPORTION = len(df.loc[df[\"label\"] == 1]) / SIZE\n",
    "\n",
    "    # calculate how large each set should be\n",
    "    SPLIT_TOTAL_SIZE = {\"train\": int(SIZE * train_size)}\n",
    "    SPLIT_TOTAL_SIZE[\"test\"] = SIZE - SPLIT_TOTAL_SIZE[\"train\"]\n",
    "\n",
    "    # define how many of each label for train set, calculated from proportion\n",
    "    SPLIT_LABEL_SIZE = {\"train\": {\"1\": int(SPLIT_TOTAL_SIZE[\"train\"] * PROPORTION)}}\n",
    "    SPLIT_LABEL_SIZE[\"train\"][\"0\"] = SPLIT_TOTAL_SIZE[\"train\"] - SPLIT_LABEL_SIZE[\"train\"][\"1\"]\n",
    "\n",
    "    # for the test set we need to calculate them from the train set to avoid mismatch in sizes between labels\n",
    "    SPLIT_LABEL_SIZE[\"test\"] = {\"1\": len(df.loc[df[\"label\"] == 1]) - SPLIT_LABEL_SIZE[\"train\"][\"1\"]}\n",
    "    SPLIT_LABEL_SIZE[\"test\"][\"0\"] = len(df.loc[df[\"label\"] == 0]) - SPLIT_LABEL_SIZE[\"train\"][\"0\"]\n",
    "    \n",
    "    print(f\"Split proportion: {SPLIT_LABEL_SIZE}\\n\\tSUM: train={sum(SPLIT_LABEL_SIZE['train'].values())}, test={sum(SPLIT_LABEL_SIZE['test'].values())}\")\n",
    "    arr_check = list()  # list just to make sure that we aren't adding posts to both train and test set\n",
    "    for current_set in [\"test\", \"train\"]:\n",
    "        df_subset = pd.DataFrame({'text': str(), 'label': int()}, index=[])\n",
    "\n",
    "        # randomly choose indices to be dropped according to proportion\n",
    "        idx_one = np.random.choice(\n",
    "            df.loc[df['label'] == 1].index,\n",
    "            size=SPLIT_LABEL_SIZE[current_set][\"1\"],\n",
    "            replace=False\n",
    "        )\n",
    "        idx_zero = np.random.choice(\n",
    "            df.loc[df['label'] == 0].index,\n",
    "            size=SPLIT_LABEL_SIZE[current_set][\"0\"],\n",
    "            replace=False\n",
    "        )\n",
    "        idx_arr = np.append(idx_one, idx_zero)\n",
    "        arr_check.append(idx_arr)\n",
    "\n",
    "        # extract rows, drop them from original df and add df_subset to result\n",
    "        rows = df.loc[idx_arr]\n",
    "        df = df.drop(idx_arr, axis=0)\n",
    "        df_subset = pd.concat((df_subset, rows)).sample(frac = 1)\n",
    "        res[current_set] = df_subset\n",
    "\n",
    "    # double check that no posts are in both sets\n",
    "    for i in arr_check[0]:\n",
    "        for j in arr_check[1]:\n",
    "            if i == j:\n",
    "                print(f\"ERROR: post present in both train and test set! [{i}]\")\n",
    "                return None\n",
    "    return {\"train\": res[\"train\"].reset_index(drop=True), \"test\": res[\"test\"].reset_index(drop=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d62084fc-bec4-4f52-b857-6aadd9715725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split proportion: {'train': {'1': 754, '0': 2478}, 'test': {'1': 189, '0': 619}}\n",
      "\tSUM: train=3232, test=808\n",
      "Split proportion: {'train': {'1': 603, '0': 1982}, 'test': {'1': 151, '0': 496}}\n",
      "\tSUM: train=2585, test=647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2585\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 647\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = test_train_split(annotated_df)\n",
    "dfs[\"test\"].to_csv(\"datasets/flashback_annotated_data_test_set.tsv\", sep=\"\\t\", index=False)\n",
    "dfs = test_train_split(dfs[\"train\"])\n",
    "dataset = DatasetDict({\"train\": Dataset.from_pandas(dfs[\"train\"]), \"test\": Dataset.from_pandas(dfs[\"test\"])})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "75e2fbbf-62d2-4f71-befb-0541962bc1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk(\"datasets/flashback_annotated_dev_set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633b056c-beda-4723-805f-9955d1468a73",
   "metadata": {},
   "source": [
    "Note that the test set is exported as a TSV, and will not be touched until the very end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae349bf9-5f59-451a-b38f-65cc310632ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import finetuned masked LM model and add text classification head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c0d605-f621-476e-8f9b-277da891a465",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "069d9b87-8837-4cd7-adb3-95527dac7631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2585\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 647\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_from_disk(\"datasets/flashback_annotated_dev_set\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4610a11d-f52a-4bc5-a112-9471dd18219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = AutoConfig.from_pretrained(\"flashback_lm_model_larger_batch_size\")\n",
    "configuration.hidden_dropout_prob = 0.2\n",
    "configuration.attention_probs_dropout_prob = 0.2\n",
    "\n",
    "masked_lm_checkpoint = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"flashback_lm_model_larger_batch_size\",\n",
    "    config=configuration\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"flashback_lm_model_larger_batch_size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae11354-8d61-475f-8ef7-5443d3f3d89f",
   "metadata": {},
   "source": [
    "The following is needed because it is wrongly set in the KB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c25628be-a9c9-42ad-baed-4997b7c0682d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length = 512  # since this is incorrectly set in KB's model\n",
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58c25e6c-9fc0-45a2-bd4a-cdec1c04204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d64666e-0afd-4863-bcdf-2a9878bea238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at datasets/flashback_annotated_dev_set/train/cache-1c80317fa3b1799d.arrow\n",
      "Loading cached processed dataset at datasets/flashback_annotated_dev_set/test/cache-bdd640fb06671ad1.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2918\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 515\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "348a7905-c6d7-4eca-bb8e-4eac02d33069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(dataset[\"train\"]) // batch_size\n",
    "metric = evaluate.load(\"f1\")\n",
    "\n",
    "model_name = \"bert-base-swedish-cased\"  # just for naming checkpoint saves\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-classification-flashback\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.0005,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    eval_accumulation_steps=1,  # slower, but less prone to overflow CUDA memory\n",
    "    num_train_epochs=3,\n",
    "    save_steps = 5000,\n",
    "    logging_steps=logging_steps,\n",
    "    save_strategy=\"no\",  # I don't need to save model checkpoints when running experiments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff92d712-bdf2-45f1-acb9-06c2635ab7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=masked_lm_checkpoint,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bcc2699f-6411-42a7-8264-bdcfa10176ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 2918\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 549\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='549' max='549' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [549/549 07:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.483600</td>\n",
       "      <td>0.442357</td>\n",
       "      <td>0.425532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.384600</td>\n",
       "      <td>0.398810</td>\n",
       "      <td>0.580087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.354900</td>\n",
       "      <td>0.416379</td>\n",
       "      <td>0.562500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 515\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 515\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 515\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=549, training_loss=0.3961040517671512, metrics={'train_runtime': 432.5692, 'train_samples_per_second': 20.237, 'train_steps_per_second': 1.269, 'total_flos': 2303274178621440.0, 'train_loss': 0.3961040517671512, 'epoch': 3.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bad4ebc3-a85c-449d-aef2-f5a04a742f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 515\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='308' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33/33 32:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.43985089659690857,\n",
       " 'eval_f1': 0.5520361990950227,\n",
       " 'eval_runtime': 21.7128,\n",
       " 'eval_samples_per_second': 23.719,\n",
       " 'eval_steps_per_second': 1.52,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08f8e967-86d5-4177-b2fe-f5900cae29f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to flashback_annotated_model\n",
      "Configuration saved in flashback_annotated_model/config.json\n",
      "Model weights saved in flashback_annotated_model/pytorch_model.bin\n",
      "tokenizer config file saved in flashback_annotated_model/tokenizer_config.json\n",
      "Special tokens file saved in flashback_annotated_model/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"flashback_annotated_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e0b4f4-423f-4f1c-bf3c-e5d4d76feab7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation of model and wrong predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a920fb-b5a9-4ba6-9ebe-d3519f0f419d",
   "metadata": {},
   "source": [
    "In order to evaluate different datasets we first define a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c6743fa-42e7-402f-919b-7c605dfc8840",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_model_pipeline = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"flashback_annotated_model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2ec730-2a58-4100-90ce-109324400060",
   "metadata": {},
   "source": [
    "Then I created a wrapper for calculating and displaying metrics, which we can easily then use for all three test sets:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c00e3c-706d-444a-9888-cb1dbd51705c",
   "metadata": {},
   "source": [
    "### Dev-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38e26dda-ac42-458e-a557-a7656d68567e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2585\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 647\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_dataset = load_from_disk(\"datasets/flashback_annotated_dev_set\")\n",
    "dev_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47dd5d9d-71bd-48c2-a434-d8ee663ab309",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_metrics = MetricsWrapper(annotated_model_pipeline, dev_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12f2c70b-9f29-4109-a3d0-a8f98adf0237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.75\n",
      "recall: 0.50\n",
      "f1_score: 0.60\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dev_metrics.display_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91d5d66-add5-407b-a100-99db1abda9c8",
   "metadata": {},
   "source": [
    "## Cross-domain scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caaba34-d840-4ef0-8525-08221ff018ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "### MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "068dbc29-8ddc-46dc-a7bc-299b34282b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at flashback_annotated_model were not used when initializing BertForMaskedLM: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at flashback_annotated_model and are newly initialized: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('flashback_annotated_model')\n",
    "tokenizer.model_max_length = 512\n",
    "model_checkpoint = AutoModelForMaskedLM.from_pretrained('flashback_annotated_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fa343a4-55ec-4465-af35-d48e7090e7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = sqlite3.connect(\"data/familjeliv.db\")\n",
    "with conn:\n",
    "    pool_df = pd.read_sql_query(\"SELECT * FROM posts;\", conn, index_col=\"id\")\n",
    "    pool_df = pool_df.sample(frac = 1, random_state = 666).reset_index(drop=True)\n",
    "    pool_df[\"label\"] = 0\n",
    "    pool_df = pool_df[[\"text\", \"label\"]]\n",
    "\n",
    "mlm_dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(pool_df[:100000].reset_index(drop=True)),\n",
    "    \"test\": Dataset.from_pandas(pool_df[100000:110000].reset_index(drop=True))\n",
    "})\n",
    "\n",
    "mlm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f779709-2826-409e-9aab-12d028d70f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(document):\n",
    "    \"\"\" returns tokenized document, quickly if tokenizer 'is fast' according to hf\"\"\"\n",
    "    result = tokenizer(document[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e9d0736-e3a2-4c90-b012-79fba6d02e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1753e3e5-74e7-444a-8d32-b248bd8ad5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_function at 0x7f381d9e2670> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb92559da284034934d8c54407e654c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1033 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34deeb48e6a14240b569f6293142dea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = mlm_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0cc3447-3535-49e8-a880-e2cfa0eb3688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53276a12-5914-41d4-88df-bc95e8e0335c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d76df5a4e841d594f7121bb5908179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8302e10515c7413baac392810588e730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 14803\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 1441\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_dataset = tokenized_datasets.map(group_texts, batched=True)\n",
    "mlm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb9d9907-b9a3-42a8-a9cd-29c6d38a172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(mlm_dataset[\"train\"]) // batch_size\n",
    "model_name = \"bert-base-swedish-cased\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-familjeliv-annotated\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    eval_accumulation_steps=1,  # slower, but less prone to overflow CUDA memory\n",
    "    num_train_epochs=15,\n",
    "    save_strategy=\"no\",  # not necessary here\n",
    "    logging_steps=logging_steps,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_checkpoint,\n",
    "    args=training_args,\n",
    "    train_dataset=mlm_dataset[\"train\"],\n",
    "    eval_dataset=mlm_dataset[\"test\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15),\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "862a3c89-4e7d-4e71-a2b0-f192676ef748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 14803\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 18510\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18510' max='18510' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18510/18510 3:50:47, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.223100</td>\n",
       "      <td>3.067340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.185600</td>\n",
       "      <td>2.646385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.907200</td>\n",
       "      <td>2.483813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.743700</td>\n",
       "      <td>2.398205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.646900</td>\n",
       "      <td>2.303165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.584000</td>\n",
       "      <td>2.252323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.528000</td>\n",
       "      <td>2.241017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.488400</td>\n",
       "      <td>2.210090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.458500</td>\n",
       "      <td>2.179126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.426200</td>\n",
       "      <td>2.178417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.406100</td>\n",
       "      <td>2.163431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.389900</td>\n",
       "      <td>2.140098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.376300</td>\n",
       "      <td>2.143870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.363300</td>\n",
       "      <td>2.135554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.358400</td>\n",
       "      <td>2.134234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1441\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1441\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1441\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1441\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1441\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1441\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1441\n",
      "  Batch size = 12\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1441\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1441\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1441\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1441\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1441\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1441\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1441\n",
      "  Batch size = 12\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18510, training_loss=2.672153714229969, metrics={'train_runtime': 13847.7417, 'train_samples_per_second': 16.035, 'train_steps_per_second': 1.337, 'total_flos': 5.845682072489472e+16, 'train_loss': 2.672153714229969, 'epoch': 15.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33b261d6-4bee-4110-bba8-b509f8953160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to familjeliv_mlm_annotated\n",
      "Configuration saved in familjeliv_mlm_annotated/config.json\n",
      "Model weights saved in familjeliv_mlm_annotated/pytorch_model.bin\n",
      "tokenizer config file saved in familjeliv_mlm_annotated/tokenizer_config.json\n",
      "Special tokens file saved in familjeliv_mlm_annotated/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"familjeliv_mlm_annotated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752bef97-d642-407a-ab52-9b8bef694797",
   "metadata": {},
   "source": [
    "Unfortunately, this seemed to reduce the models performance in the context of cross-domain scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcec51e-6f31-4aa4-9da7-b966672c32a8",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8557dc5-f4a4-45d8-8548-35492fdf5ed2",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "843860f1-54a8-4a5c-a7b1-f7ea3cc95d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flashback_testset = pd.read_csv(\"datasets/flashback_annotated_data_test_set.tsv\", sep=\"\\t\")\n",
    "flashback_testset = Dataset.from_pandas(flashback_testset)\n",
    "flashback_testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aca316ce-1481-47ae-9ee9-09a4cfbd6eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_metrics = MetricsWrapper(annotated_model_pipeline, flashback_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c00c517c-6fb8-40c1-8c7f-ad5cb28026f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.79\n",
      "recall: 0.60\n",
      "f1_score: 0.68\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "baseline_metrics.display_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23873c8b-db68-42da-bf57-45f733899cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"metrics/flashback_annotated_metrics.pkl\", \"wb\") as f:\n",
    "    pickle.dump(baseline_metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae78e82-0820-49f1-bb7e-bc91f7a25797",
   "metadata": {},
   "source": [
    "### Cross-domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53c9d6de-5416-488e-bff9-dcaf2ee0dff0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "familjeliv_automatic_pipeline = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"familjeliv_mlm_annotated\",\n",
    ")\n",
    "familjeliv_testset = DatasetDict.load_from_disk(\"datasets/familjeliv_testset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d864e54a-db21-445e-a857-97b504985d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "familjeliv_testset_metrics = MetricsWrapper(familjeliv_automatic_pipeline, familjeliv_testset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52dad6de-7682-4c01-8122-c85df4e4f164",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"metrics/familjeliv_annotated_metrics.pkl\", \"wb\") as f:\n",
    "    pickle.dump(familjeliv_testset_metrics, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc-autonumbering": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
