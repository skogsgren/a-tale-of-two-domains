{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09fa07a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from transformers import AdamW, get_scheduler\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b967d435-3614-4670-a4f4-28de06d7ef19",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import data for converting to 游뱅 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "096d91fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "annotated_df = pd.read_csv(\"datasets/flashback_annotated.tsv\", sep=\"\\t\", index_col=0)\n",
    "unannotated_df = pd.read_csv(\"datasets/flashback_raw.tsv\", sep=\"\\t\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd6315d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Finetune for masked LM task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15922e11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf8197585be4bfe894b79b68f528b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d3fbee76684c3da980002ab73849ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/491 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e5133f66772419da54fbc2ee1f3c227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/390k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3a36842b9345d4aa0402703e860fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1f45984bdf48c0af09d1838466d4d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac6d82da3e845cd975cbe40fb95176c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('KB/bert-base-swedish-cased')\n",
    "model_checkpoint = AutoModelForMaskedLM.from_pretrained('KB/bert-base-swedish-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b59a356-88ec-40a9-94fc-5f86a7e3d24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_mask_filler = pipeline(\n",
    "    \"fill-mask\", model=model_checkpoint, tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef8c6b8c-02e1-4e25-97ae-2cc3b16d3359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.16540421545505524,\n",
       "  'token': 38404,\n",
       "  'token_str': 'araber',\n",
       "  'sequence': 'Vad hj칛lper det att Paris har s친 otroligt mycket vackert och kul att uppleva, n칛r det st친r araber och araber precis 칬ver allt.'},\n",
       " {'score': 0.08115306496620178,\n",
       "  'token': 17319,\n",
       "  'token_str': 'muslimer',\n",
       "  'sequence': 'Vad hj칛lper det att Paris har s친 otroligt mycket vackert och kul att uppleva, n칛r det st친r araber och muslimer precis 칬ver allt.'},\n",
       " {'score': 0.06851934641599655,\n",
       "  'token': 32241,\n",
       "  'token_str': 'fransm칛n',\n",
       "  'sequence': 'Vad hj칛lper det att Paris har s친 otroligt mycket vackert och kul att uppleva, n칛r det st친r araber och fransm칛n precis 칬ver allt.'},\n",
       " {'score': 0.04426249861717224,\n",
       "  'token': 13410,\n",
       "  'token_str': 'judar',\n",
       "  'sequence': 'Vad hj칛lper det att Paris har s친 otroligt mycket vackert och kul att uppleva, n칛r det st친r araber och judar precis 칬ver allt.'},\n",
       " {'score': 0.0346917100250721,\n",
       "  'token': 38381,\n",
       "  'token_str': 'europ칠er',\n",
       "  'sequence': 'Vad hj칛lper det att Paris har s친 otroligt mycket vackert och kul att uppleva, n칛r det st친r araber och europ칠er precis 칬ver allt.'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_mask_filler(\"Vad hj칛lper det att Paris har s친 otroligt mycket vackert och kul att uppleva, n칛r det st친r araber och [MASK] precis 칬ver allt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6520d1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Testing default masked LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3071807c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[MASK]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.mask_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99d08a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,   361, 19134,    48,  1921,     4,   181,   408,     7,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"Jag hatar att kvinnor [MASK] s친 mycket.\", return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "600528c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Jag hatar att kvinnor [MASK] s친 mycket. [SEP]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([tokenizer.decode(x) for x in inputs['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b641c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74ba2ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.5930, -2.4002, -3.3570,  ..., -5.7172, -6.7367, -7.6456],\n",
       "         [-3.4565, -1.4567, -3.5432,  ..., -1.2682, -6.0981, -2.3997],\n",
       "         [-2.4656,  2.2299, -5.2629,  ..., -1.9827, -0.9353, -2.6023],\n",
       "         ...,\n",
       "         [-9.2317,  0.7863, -3.6915,  ..., -6.0307, -7.7294, -9.9168],\n",
       "         [-2.9332,  2.2197, -0.8976,  ..., -2.6394, -2.0116, -3.4200],\n",
       "         [-5.9556, -2.4276, -2.0761,  ..., -5.2691, -6.6226, -7.1704]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_logits = model_checkpoint(**inputs).logits\n",
    "token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "115970dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.1517,  2.8392, -2.8566,  ..., -2.6504, -3.2629, -6.0682]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "mask_token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "801eca17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jag hatar att kvinnor br친kar s친 mycket.\n",
      "Jag hatar att kvinnor pratar s친 mycket.\n",
      "Jag hatar att kvinnor gr친ter s친 mycket.\n",
      "Jag hatar att kvinnor dricker s친 mycket.\n",
      "Jag hatar att kvinnor lider s친 mycket.\n"
     ]
    }
   ],
   "source": [
    "# Pick the <mask> candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "for mask_word in [tokenizer.decode([x]) for x in top_5_tokens]:\n",
    "    print(f\"Jag hatar att kvinnor {mask_word} s친 mycket.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60e495d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_masked_lm(text):\n",
    "    inputs = tokenizer(\"%s [MASK].\" % text, return_tensors=\"pt\")\n",
    "    token_logits = model_checkpoint(**inputs).logits\n",
    "    mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "    top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "    for mask_word in [tokenizer.decode([x]) for x in top_5_tokens]:\n",
    "        print(\"%s %s.\" %(text, mask_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b98ae63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jag hatar att kvinnor gr친ter.\n",
      "Jag hatar att kvinnor drabbas.\n",
      "Jag hatar att kvinnor ljuger.\n",
      "Jag hatar att kvinnor dricker.\n",
      "Jag hatar att kvinnor sl친ss.\n"
     ]
    }
   ],
   "source": [
    "test_masked_lm(\"Jag hatar att kvinnor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95c952b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jag hatar att m칛n ljuger.\n",
      "Jag hatar att m칛n br친kar.\n",
      "Jag hatar att m칛n gr친ter.\n",
      "Jag hatar att m칛n sl친ss.\n",
      "Jag hatar att m칛n dricker.\n"
     ]
    }
   ],
   "source": [
    "test_masked_lm(\"Jag hatar att m칛n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c29c920-b8c7-446f-971a-02e6af6eb7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invandrare b칬r skyddas.\n",
      "Invandrare b칬r utvisas.\n",
      "Invandrare b칬r prioriteras.\n",
      "Invandrare b칬r avvisas.\n",
      "Invandrare b칬r uppmuntras.\n"
     ]
    }
   ],
   "source": [
    "test_masked_lm(\"Invandrare b칬r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b47776",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create 游뱅 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02ddd448",
   "metadata": {},
   "outputs": [],
   "source": [
    "unannotated_df = unannotated_df.sample(frac = 1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b803fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_lm_test_df = unannotated_df[:10000].reset_index(drop=True)\n",
    "masked_lm_train_df = unannotated_df[10000:110000].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a71185ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_lm_dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(masked_lm_train_df),\n",
    "    \"test\": Dataset.from_pandas(masked_lm_test_df)\n",
    "})\n",
    "\n",
    "masked_lm_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4b9bc8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Convert dataset to chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9aef332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(document):\n",
    "    \"\"\" returns tokenized document, quickly if tokenizer 'is fast' according to hf\"\"\"\n",
    "    result = tokenizer(document[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d8e874a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000000000000019884624838656"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b36c88",
   "metadata": {},
   "source": [
    "This seems like an error, so I'll redefine it according to the BERT specification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34a76d48-e451-406e-9576-97052c110f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length = 512\n",
    "chunk_size = tokenizer.model_max_length\n",
    "chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8a55bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_function at 0x7f439c29b310> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0701f63c3e0e4772904f800f3c856552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1180 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71531430349425898770b9b13244c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = masked_lm_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92a7e536",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_samples = tokenized_datasets[\"train\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "538b6483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum: 929\n",
      "Concatenated length: 929\n"
     ]
    }
   ],
   "source": [
    "print(\"Sum: %d\" %sum([len(x) for x in tokenized_samples[\"input_ids\"]]))\n",
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"Concatenated length: {total_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ef4e5e-b583-4f40-a89f-a7e0eb6490ea",
   "metadata": {},
   "source": [
    "These are identical, which is what we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "922a8287",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = {\n",
    "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2265c9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Chunk length: 512'\n",
      "'>>> Chunk length: 417'\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bc5103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34f1dd06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "621f8fe074d8442ebe6e4e89e4b162bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59304e05ff7140a1b8a561dc9a821e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 17371\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 1711\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dataset = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95007285",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(lm_dataset[\"train\"][10][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cadbb06-2181-463a-bc70-f739a1e95f95",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bde03c5",
   "metadata": {},
   "source": [
    "#### 游뱅 Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42281dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9545fed5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "samples = [lm_dataset[\"train\"][i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43441788",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(lm_dataset[\"train\"]) // batch_size\n",
    "model_name = \"bert-base-swedish-cased\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-flashback\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    eval_accumulation_steps=1,  # slower, but less prone to overflow CUDA memory\n",
    "    num_train_epochs=15,\n",
    "    save_steps = 5000,\n",
    "    logging_steps=logging_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c6657b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model_checkpoint,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb592439-0c6a-415b-b7a5-c0ec4fda8760",
   "metadata": {},
   "source": [
    "##### Training using trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d49cca3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1711\n",
      "  Batch size = 12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='143' max='143' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [143/143 01:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 36.28\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21711da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 17318\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 12\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 21660\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21660' max='21660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21660/21660 4:33:50, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.393500</td>\n",
       "      <td>2.163418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.272300</td>\n",
       "      <td>2.127420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>2.094575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.172200</td>\n",
       "      <td>2.067284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.142200</td>\n",
       "      <td>2.061248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.118800</td>\n",
       "      <td>2.048880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.088900</td>\n",
       "      <td>2.019770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.074000</td>\n",
       "      <td>2.025061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.064200</td>\n",
       "      <td>2.003407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.045500</td>\n",
       "      <td>2.004086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.031600</td>\n",
       "      <td>1.999206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.018500</td>\n",
       "      <td>2.007925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.011000</td>\n",
       "      <td>1.992372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.002900</td>\n",
       "      <td>1.966620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.000600</td>\n",
       "      <td>1.984422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1727\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1727\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1727\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to bert-base-swedish-cased-finetuned-flashback/checkpoint-5000\n",
      "Configuration saved in bert-base-swedish-cased-finetuned-flashback/checkpoint-5000/config.json\n",
      "Model weights saved in bert-base-swedish-cased-finetuned-flashback/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-swedish-cased-finetuned-flashback/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-swedish-cased-finetuned-flashback/checkpoint-5000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1727\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1727\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1727\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1727\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1727\n",
      "  Batch size = 12\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1727\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1727\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1727\n",
      "  Batch size = 12\n",
      "Saving model checkpoint to bert-base-swedish-cased-finetuned-flashback/checkpoint-20000\n",
      "Configuration saved in bert-base-swedish-cased-finetuned-flashback/checkpoint-20000/config.json\n",
      "Model weights saved in bert-base-swedish-cased-finetuned-flashback/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-swedish-cased-finetuned-flashback/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-swedish-cased-finetuned-flashback/checkpoint-20000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1727\n",
      "  Batch size = 12\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1727\n",
      "  Batch size = 12\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=21660, training_loss=2.110331358341629, metrics={'train_runtime': 16431.4322, 'train_samples_per_second': 15.809, 'train_steps_per_second': 1.318, 'total_flos': 6.838851728120832e+16, 'train_loss': 2.110331358341629, 'epoch': 15.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ceca9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1727\n",
      "  Batch size = 12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='144' max='144' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [144/144 00:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 7.34\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "169b96d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to flashback_lm_model_larger_batch_size\n",
      "Configuration saved in flashback_lm_model_larger_batch_size/config.json\n",
      "Model weights saved in flashback_lm_model_larger_batch_size/pytorch_model.bin\n",
      "tokenizer config file saved in flashback_lm_model_larger_batch_size/tokenizer_config.json\n",
      "Special tokens file saved in flashback_lm_model_larger_batch_size/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"flashback_lm_model_larger_batch_size\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
